{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trump Tweet Bot (Keras)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lab_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext lab_black\n"
     ]
    }
   ],
   "source": [
    "%load_ext lab_black\n",
    "import sys, random, os, gc, re, json\n",
    "from string import punctuation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Input, Dense\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "\n",
    "from eda_utils import tweet_cleaner, rmv_uncommon\n",
    "from bot_utils import DataGenerator, TextCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = \"./data/archive.json\"\n",
    "\n",
    "df = pd.read_json(DATA_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].apply(tweet_cleaner)\n",
    "df[\"text\"] = df.text.apply(rmv_uncommon)\n",
    "# Remove tweets that just contain empty strings\n",
    "df = df[(df[\"text\"] != \"\") | (df[\"text\"] != \" \")]\n",
    "\n",
    "# Remove Retweets\n",
    "df = df[df.is_retweet == False]\n",
    "\n",
    "# Use \"special\" symbol @ to indicate end of tweet, since we removed them all before\n",
    "corpus = \"@\".join(df[\"text\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Set-up\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"test_bot1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm (CuDNNLSTM)       (None, 32, 256)           346112    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 256)           0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 256)               526336    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 80)                20560     \n",
      "=================================================================\n",
      "Total params: 893,008\n",
      "Trainable params: 893,008\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "full_corp = TextCorpus(corpus)\n",
    "\n",
    "train_corp = full_corp[: int(len(full_corp) * 0.9)]\n",
    "test_corp = full_corp[int(len(full_corp) * 0.9) :]\n",
    "\n",
    "SAMPLE_LEN = 32\n",
    "STEP_SIZE = 3\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_gen = DataGenerator(train_corp, SAMPLE_LEN, STEP_SIZE)\n",
    "test_gen = DataGenerator(test_corp, SAMPLE_LEN, STEP_SIZE, shuffle=False)\n",
    "\n",
    "nchars = full_corp.get_num_chars()\n",
    "\n",
    "model = Sequential(name=\"test_bot1\")\n",
    "model.add(Input(shape=(SAMPLE_LEN, nchars), dtype=np.float32))\n",
    "model.add(CuDNNLSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(CuDNNLSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(nchars, activation=\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(arr, corp):\n",
    "\n",
    "    for batch in range(arr[0].shape[0]):\n",
    "        print(\"-\" * 40)\n",
    "        print(\"Batch \" + str(batch))\n",
    "        string = \"\"\n",
    "        sample = arr[0][batch, ...]\n",
    "        for letter in sample:\n",
    "            string += corp.indicies_to_char[np.where(letter == 1)[0][0]]\n",
    "        ans = corp.indicies_to_char[np.where(arr[1][batch] == 1)[0][0]]\n",
    "        print(\"SAMPLE:\", string)\n",
    "        print(\"TARGET: ({})\".format(ans))\n",
    "\n",
    "\n",
    "# decode(train_gen[0], full_corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0\n",
      "----------------------------------------\n",
      "Batch 0\n",
      "SAMPLE: I would like to wish everyone A \n",
      "TARGET: (H)\n",
      "----------------------------------------\n",
      "Batch 1\n",
      "SAMPLE: ould like to wish everyone A HAP\n",
      "TARGET: (P)\n",
      "----------------------------------------\n",
      "Batch 2\n",
      "SAMPLE: d like to wish everyone A HAPPY \n",
      "TARGET: (A)\n",
      "----------------------------------------\n",
      "Batch 3\n",
      "SAMPLE: ike to wish everyone A HAPPY AND\n",
      "TARGET: ( )\n",
      "----------------------------------------\n",
      "Batch 4\n",
      "SAMPLE:  to wish everyone A HAPPY AND HE\n",
      "TARGET: (A)\n",
      "----------------------------------------\n",
      "Batch 5\n",
      "SAMPLE:  wish everyone A HAPPY AND HEALT\n",
      "TARGET: (H)\n",
      "----------------------------------------\n",
      "Batch 6\n",
      "SAMPLE: sh everyone A HAPPY AND HEALTHY \n",
      "TARGET: (N)\n",
      "----------------------------------------\n",
      "Batch 7\n",
      "SAMPLE: everyone A HAPPY AND HEALTHY NEW\n",
      "TARGET: ( )\n",
      "----------------------------------------\n",
      "Batch 8\n",
      "SAMPLE: ryone A HAPPY AND HEALTHY NEW YE\n",
      "TARGET: (A)\n",
      "----------------------------------------\n",
      "Batch 9\n",
      "SAMPLE: ne A HAPPY AND HEALTHY NEW YEAR.\n",
      "TARGET: ( )\n",
      "----------------------------------------\n",
      "Batch 10\n",
      "SAMPLE: A HAPPY AND HEALTHY NEW YEAR. WE\n",
      "TARGET: ( )\n",
      "----------------------------------------\n",
      "Batch 11\n",
      "SAMPLE: APPY AND HEALTHY NEW YEAR. WE MU\n",
      "TARGET: (S)\n",
      "----------------------------------------\n",
      "Batch 12\n",
      "SAMPLE: Y AND HEALTHY NEW YEAR. WE MUST \n",
      "TARGET: (A)\n",
      "----------------------------------------\n",
      "Batch 13\n",
      "SAMPLE: ND HEALTHY NEW YEAR. WE MUST ALL\n",
      "TARGET: ( )\n",
      "----------------------------------------\n",
      "Batch 14\n",
      "SAMPLE: HEALTHY NEW YEAR. WE MUST ALL WO\n",
      "TARGET: (R)\n",
      "----------------------------------------\n",
      "Batch 15\n",
      "SAMPLE: LTHY NEW YEAR. WE MUST ALL WORK \n",
      "TARGET: (T)\n",
      "----------------------------------------\n",
      "Batch 16\n",
      "SAMPLE: Y NEW YEAR. WE MUST ALL WORK TOG\n",
      "TARGET: (E)\n",
      "----------------------------------------\n",
      "Batch 17\n",
      "SAMPLE: EW YEAR. WE MUST ALL WORK TOGETH\n",
      "TARGET: (E)\n",
      "----------------------------------------\n",
      "Batch 18\n",
      "SAMPLE: YEAR. WE MUST ALL WORK TOGETHER \n",
      "TARGET: (T)\n",
      "----------------------------------------\n",
      "Batch 19\n",
      "SAMPLE: R. WE MUST ALL WORK TOGETHER TO,\n",
      "TARGET: ( )\n",
      "----------------------------------------\n",
      "Batch 20\n",
      "SAMPLE: WE MUST ALL WORK TOGETHER TO, FI\n",
      "TARGET: (N)\n",
      "----------------------------------------\n",
      "Batch 21\n",
      "SAMPLE: MUST ALL WORK TOGETHER TO, FINAL\n",
      "TARGET: (L)\n",
      "----------------------------------------\n",
      "Batch 22\n",
      "SAMPLE: T ALL WORK TOGETHER TO, FINALLY,\n",
      "TARGET: ( )\n",
      "----------------------------------------\n",
      "Batch 23\n",
      "SAMPLE: LL WORK TOGETHER TO, FINALLY, MA\n",
      "TARGET: (K)\n",
      "----------------------------------------\n",
      "Batch 24\n",
      "SAMPLE: WORK TOGETHER TO, FINALLY, MAKE \n",
      "TARGET: (A)\n",
      "----------------------------------------\n",
      "Batch 25\n",
      "SAMPLE: K TOGETHER TO, FINALLY, MAKE AME\n",
      "TARGET: (R)\n",
      "----------------------------------------\n",
      "Batch 26\n",
      "SAMPLE: OGETHER TO, FINALLY, MAKE AMERIC\n",
      "TARGET: (A)\n",
      "----------------------------------------\n",
      "Batch 27\n",
      "SAMPLE: THER TO, FINALLY, MAKE AMERICA S\n",
      "TARGET: (A)\n",
      "----------------------------------------\n",
      "Batch 28\n",
      "SAMPLE: R TO, FINALLY, MAKE AMERICA SAFE\n",
      "TARGET: ( )\n",
      "----------------------------------------\n",
      "Batch 29\n",
      "SAMPLE: O, FINALLY, MAKE AMERICA SAFE AG\n",
      "TARGET: (A)\n",
      "----------------------------------------\n",
      "Batch 30\n",
      "SAMPLE: FINALLY, MAKE AMERICA SAFE AGAIN\n",
      "TARGET: ( )\n",
      "----------------------------------------\n",
      "Batch 31\n",
      "SAMPLE: ALLY, MAKE AMERICA SAFE AGAIN AN\n",
      "TARGET: (D)\n"
     ]
    }
   ],
   "source": [
    "for step, (x, y) in enumerate(train_gen):\n",
    "    print(\"Step: \", step)\n",
    "    decode((x, y), full_corp)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bot_func import one_hot_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_features(start_inds, data, sample_len, nchars):\n",
    "    \"\"\"\n",
    "    Generates the one hot encoded feature and target arrays.\n",
    "    As it's a nested for-loop invoving just numpy it's massivly more\n",
    "    performant to compile it with numba.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    start_inds, np.ndarray, (batch_size,)\n",
    "        Array of starting indices for the samples in this batch\n",
    "\n",
    "    data, np.ndarray, (corpus_size, )\n",
    "        The full corpus of text in ordinal form\n",
    "\n",
    "    sample_len, int\n",
    "        Number of characters before target character\n",
    "\n",
    "    nchars, int\n",
    "        Number of unique characters in corpus and the size of each one hot array\n",
    "    \"\"\"\n",
    "    out = np.zeros((len(start_inds), sample_len + 1, nchars), np.float32)\n",
    "    for j, start_ind in enumerate(start_inds):\n",
    "        for k, val in enumerate(np.arange(start_ind, start_ind + sample_len + 1)):\n",
    "            out[j, k, data[val]] = 1\n",
    "    return out[:, :-1, :], out[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would like to wish everyone A H\n",
      "[35  0 76 68 74 65 57  0 65 62 64 58  0 73 68  0 76 62 72 61  0 58 75 58\n",
      " 71 78 68 67 58  0 27  0 34]\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]] (1, 32, 80)\n",
      "----------------------------------------\n",
      "Batch 0\n",
      "SAMPLE: I would like to wish everyone A \n",
      "TARGET: (H)\n"
     ]
    }
   ],
   "source": [
    "cx = corpus[:33]\n",
    "print(cx)\n",
    "ox = full_corp.encode_numerical(cx)\n",
    "print(ox)\n",
    "ohf, oht = one_hot_features(np.array([0]), ox, 32, full_corp.get_num_chars())\n",
    "print(ohf, ohf.shape)\n",
    "decode((ohf, oht), full_corp)\n",
    "# x, y = one_hot_features(np.array([0]), x, 32, full_corp.get_num_chars())\n",
    "# print(x)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an Predict\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Epoch 1\n"
     ]
    }
   ],
   "source": [
    "tweet_ends = np.where(np.asarray(list(corpus)) == \"@\")[0]\n",
    "\n",
    "for epoch in range(1, 31):\n",
    "    print('-' * 40)\n",
    "    print('Epoch', epoch)\n",
    "    model.fit(\n",
    "        train_gen,\n",
    "        steps_per_epoch=train_gen.epoch_size,\n",
    "        epochs=1,\n",
    "        validation_data=test_gen,\n",
    "        validation_steps=test_gen.epoch_size)\n",
    "    \n",
    "    seed_index = 1 + np.random.choice(tweet_ends, 1)[0]\n",
    "    for diversity in [0.2, 0.7, 1.2]:\n",
    "        genertate_tweet(model, seed_index, diversity, full_corp)\n",
    "    print('-' * 40)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
